{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15036\n",
      "15036\n",
      "['0000200100.xml.txt', '0000200200.xml.txt', '0000200400.xml.txt', '0000200500.xml.txt', '0000200601.xml.txt']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Now, getting all filenames from Fiction_Big for calculating feature-vectors.\n",
    "'''\n",
    "import pandas as pd\n",
    "fiction_big = pd.read_csv('./Fiction_Big.csv')\n",
    "fn_fiction_big = fiction_big['Filename'].tolist()\n",
    "print len(fn_fiction_big)\n",
    "fn_fiction_big = [f + '.txt' for f in fn_fiction_big]\n",
    "print len(fn_fiction_big)\n",
    "print fn_fiction_big[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 154924 files in our dataset.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Checking if all 15,036 files are present in my dataset.\n",
    "'''\n",
    "import os\n",
    "\n",
    "# Adding all filenames in the dataset to one list:\n",
    "path = '../Dataset/'\n",
    "allFilenamesInDataset = []\n",
    "folders = os.listdir(path)[1:]\n",
    "for folder_name in folders:\n",
    "    allFilenamesInDataset.extend(os.listdir(path+folder_name))\n",
    "print \"There are \" + str(len(allFilenamesInDataset)) + \" files in our dataset.\"\n",
    "\n",
    "for fn in fn_fiction_big:\n",
    "    if fn not in allFilenamesInDataset:\n",
    "        print fn\n",
    "print \"Done.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4558\n",
      "665\n",
      "4558\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Checking the intersection of Fiction_Small and Fiction_Big:\n",
    "'''\n",
    "fnames_fiction_small = pd.read_csv('./Fiction_Small.csv')['Filename'].tolist()\n",
    "small_and_big_intersection = fiction_big.loc[fiction_big['Filename'].isin(fnames_fiction_small)]\n",
    "intersection_fnames = small_and_big_intersection['Filename'].tolist()\n",
    "print len(fnames_fiction_small)\n",
    "# There are 3893 files in ans; Hence, some of the fiction_small is not in fiction_big. Finding out which ones:\n",
    "notInBig = set(fnames_fiction_small) - set(intersection_fnames)\n",
    "print len(notInBig)\n",
    "print len(notInBig) + len(intersection_fnames)\n",
    "# print notInBig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are 665 documents that are in fiction_small, but not in fiction_big; Present in the list notInBig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Mapping each filename to its folder.\n",
    "'''\n",
    "# Generating a dictionary key to map filenames to folder names- Key: GenRef; Value: list of all filenames in GenRef.\n",
    "key_to_txts = {}\n",
    "for folder_name in folders:\n",
    "    temp = os.listdir(path+folder_name)\n",
    "    key_to_txts[folder_name] = temp\n",
    "\n",
    "# Mapping each filename to the folder name:\n",
    "fiction_FolderNames = []\n",
    "for fname in fn_fiction_big:\n",
    "    for folder in key_to_txts.keys():\n",
    "        if fname in key_to_txts[folder]:\n",
    "            fiction_FolderNames.append(folder)\n",
    "\n",
    "print \"They come from: \", set(fiction_FolderNames)\n",
    "\n",
    "# fiction is a list of tuples; first element of tuple is the txt filename, second element is its folder name.\n",
    "fiction = zip(fn_fiction_big, fiction_FolderNames)\n",
    "print fiction[:5]\n",
    "\n",
    "# Pickling the list 'fiction' because it matters for the ordering:\n",
    "import pickle\n",
    "\n",
    "with open('./order_fiction.pickle', 'wb') as f:\n",
    "    pickle.dump(fiction, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Calculating feature-vectors for each filename in the test set (Fiction_Big).\n",
    "'''\n",
    "\n",
    "# NOTE: Calculating feature-vectors assuming Model 2.\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from collections import defaultdict\n",
    "from heapq import nlargest\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import unicodecsv as csv\n",
    "\n",
    "'''\n",
    "Takes in a list of sentences where each sentence is a list of words, and optional argument 'user_stopwords'.\n",
    "Returns a dictionary with each 'word' is the key, and 'count' as the value.\n",
    "'''\n",
    "def calculate_frequencies(sentences_ll):  # sentences_ll is a list of lists\n",
    "    frequency = defaultdict(int)    # default value : 0\n",
    "    \n",
    "    for sentence in sentences_ll:\n",
    "        for word in sentence:\n",
    "            word = word.lower()\n",
    "            \n",
    "            # Case I: No stopwords; Just one condition: len > 3\n",
    "            if len(word) > 3:\n",
    "                frequency[word] += 1\n",
    "\n",
    "    return frequency\n",
    "\n",
    "'''\n",
    "Takes in text, and n = number of features\n",
    "Returns a list of n most frequent words\n",
    "'''\n",
    "def get_features(text, n):  # n is the desired no. of features\n",
    "    sentences = sent_tokenize(text.decode('utf8'))\n",
    "    \n",
    "    sentences_ll = []\n",
    "    for s in sentences:\n",
    "        words = word_tokenize(s)\n",
    "        sentences_ll.append(words)\n",
    "\n",
    "    frequency = calculate_frequencies(sentences_ll)\n",
    "    return nlargest(n, frequency, key=frequency.get)\n",
    "\n",
    "\n",
    "def run_and_pickle(nf):\n",
    "    numberOfFeatures = nf\n",
    "    print \"Running for\", nf\n",
    "    # They are a list of lists where each list represents a document as a collection of n frequent words.\n",
    "    features_fiction = []\n",
    "\n",
    "    print \"Fiction:\"\n",
    "    k = 0\n",
    "    for (n, folder) in fiction:\n",
    "        if k % 500 == 0:\n",
    "            print k\n",
    "        k += 1\n",
    "        with open('../Dataset/' + folder + '/' + n) as f:\n",
    "            text = f.read()\n",
    "            features_fiction.append(get_features(text, numberOfFeatures))\n",
    "\n",
    "    # Pickling the results:\n",
    "    with open('./features/test_big_fiction_'+str(numberOfFeatures)+'.pickle', 'wb') as f:\n",
    "        pickle.dump(features_fiction, f)\n",
    "    \n",
    "\n",
    "    # CSV-ing the results:\n",
    "    with open('./features/testbigfiction_'+str(numberOfFeatures)+'.csv', 'wb') as f:\n",
    "        writer = csv.writer(f, encoding='utf-8')\n",
    "        writer.writerows(features_fiction)\n",
    "\n",
    "    return features_fiction\n",
    "        \n",
    "features_fiction_big = run_and_pickle(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15036, 154749)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Calculating CountVec feature vectors for test-fiction-big.\n",
    "'''\n",
    "\n",
    "with open('./features/test_big_fiction_500.pickle', 'rb') as f:\n",
    "    features_fiction_big = pickle.load(f)\n",
    "\n",
    "# Getting it ready for Count Vectorizer:\n",
    "countvec_500 = []\n",
    "\n",
    "for doc in features_fiction_big:\n",
    "    temp = ' '.join(doc)\n",
    "    countvec_500.append(temp)\n",
    "    \n",
    "Xtest_500 = vectorizer500.transform(countvec_500)\n",
    "\n",
    "print Xtest_500.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
