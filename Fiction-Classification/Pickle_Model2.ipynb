{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fiction comes from:  set(['LitAndLang_2', 'SSAndFineArt', 'HistAndGeo', 'MedSciTech', 'Law', 'RelAndPhil', 'LitAndLang_1'])\n",
      "Non-Fiction comes from:  set(['RelAndPhil', 'SSAndFineArt', 'HistAndGeo', 'MedSciTech', 'Law', 'LitAndLang_2', 'LitAndLang_1'])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "train_fiction_fnames = pd.read_csv('./Fiction_Small.csv')['Filename'].tolist()\n",
    "train_nonfiction_fnames = pd.read_csv('./NonFiction.csv')[:]['Filename'].tolist()\n",
    "\n",
    "# Shuffling non-fiction and then picking 4558 samples for our analysis:\n",
    "random.seed(7)\n",
    "random.shuffle(train_nonfiction_fnames)\n",
    "train_nonfiction_fnames = train_nonfiction_fnames[:4558]\n",
    "\n",
    "# Adding .txt to each filename:\n",
    "train_fiction = [f + '.txt' for f in train_fiction_fnames]\n",
    "train_nonfiction = [f + '.txt' for f in train_nonfiction_fnames]\n",
    "\n",
    "# Adding all filenames in the dataset to one list:\n",
    "path = '../Dataset/'\n",
    "allFilenamesInDataset = []\n",
    "folders = os.listdir(path)[1:]\n",
    "for folder_name in folders:\n",
    "    allFilenamesInDataset.extend(os.listdir(path+folder_name))\n",
    "# print \"There are \" + str(len(allFilenamesInDataset)) + \" files in our dataset.\"\n",
    "\n",
    "# Generating a dictionary key to map filenames to folder names- Key: GenRef; Value: list of all filenames in GenRef.\n",
    "key_to_txts = {}\n",
    "for folder_name in folders:\n",
    "    temp = os.listdir(path+folder_name)\n",
    "    key_to_txts[folder_name] = temp\n",
    "    \n",
    "    \n",
    "    \n",
    "# Mapping each filename to the folder name:\n",
    "\n",
    "# For Fiction:\n",
    "fiction_train_FolderNames = []\n",
    "for fname in train_fiction:\n",
    "    for folder in key_to_txts.keys():\n",
    "        if fname in key_to_txts[folder]:\n",
    "            fiction_train_FolderNames.append(folder)\n",
    "# print len(fiction_train_FolderNames)\n",
    "\n",
    "# For Non-Fiction:\n",
    "non_fiction_train_FolderNames = []\n",
    "for fname in train_nonfiction:\n",
    "    for folder in key_to_txts.keys():\n",
    "        if fname in key_to_txts[folder]:\n",
    "            non_fiction_train_FolderNames.append(folder)\n",
    "# print len(non_fiction_train_FolderNames)\n",
    "\n",
    "print \"Fiction comes from: \", set(fiction_train_FolderNames)\n",
    "print \"Non-Fiction comes from: \", set(non_fiction_train_FolderNames)\n",
    "\n",
    "# fiction/nonfiction is a list of tuples; first element of tuple is the txt filename, second element is its folder name.\n",
    "fiction = zip(train_fiction, fiction_train_FolderNames)\n",
    "nonfiction = zip(train_nonfiction, non_fiction_train_FolderNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from collections import defaultdict\n",
    "from heapq import nlargest\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "\n",
    "def init_stopwords():\n",
    "    stopwords_path = '/Users/sunyambagga/nltk_data/corpora/stopwords'\n",
    "    \n",
    "    # Language stopwords:\n",
    "    lang_stopwords = []\n",
    "    for lang in os.listdir(stopwords_path):\n",
    "        if lang != 'README':\n",
    "            lang_stopwords.extend((stopwords.words(lang)))\n",
    "\n",
    "    # By Prof. Andrew:\n",
    "    EnglishNames = pd.read_csv('../Document Classification 4.0/stopwords_prof/Dict_English_NamesPlus.csv', header=None)[0].tolist()\n",
    "    FrenchNames = pd.read_csv('../Document Classification 4.0/stopwords_prof/Dict_French_NamesPlus.csv', header=None)[0].tolist()\n",
    "\n",
    "    all_stopwords = lang_stopwords + EnglishNames + FrenchNames + list(punctuation)\n",
    "    all_stopwords = set(all_stopwords)\n",
    "    \n",
    "    return all_stopwords\n",
    "\n",
    "\n",
    "'''\n",
    "Takes in a list of sentences where each sentence is a list of words, and optional argument 'user_stopwords'.\n",
    "Returns a dictionary with each 'word' is the key, and 'count' as the value.\n",
    "'''\n",
    "def calculate_frequencies(sentences_ll, user_stopwords=None):  # sentences_ll is a list of lists\n",
    "    frequency = defaultdict(int)    # default value : 0\n",
    "    \n",
    "    all_stopwords = init_stopwords()\n",
    "    if user_stopwords is not None:\n",
    "        final_stopwords = set(user_stopwords).union(all_stopwords)\n",
    "    else:\n",
    "        final_stopwords = all_stopwords\n",
    "    \n",
    "    for sentence in sentences_ll:\n",
    "        for word in sentence:\n",
    "            word = word.lower()\n",
    "            \n",
    "            # Case I: No stopwords; Just one condition: len > 3\n",
    "            if len(word) > 3:\n",
    "                frequency[word] += 1\n",
    "\n",
    "    return frequency\n",
    "\n",
    "'''\n",
    "Takes in text, and n = number of features\n",
    "Returns a list of n most frequent words\n",
    "'''\n",
    "def get_features(text, n, user_stopwords=None):  # n is the desired no. of features\n",
    "    sentences = sent_tokenize(text.decode('utf8'))\n",
    "    \n",
    "    sentences_ll = []\n",
    "    for s in sentences:\n",
    "        words = word_tokenize(s)\n",
    "        sentences_ll.append(words)\n",
    "\n",
    "    frequency = calculate_frequencies(sentences_ll, user_stopwords)\n",
    "    return nlargest(n, frequency, key=frequency.get)\n",
    "\n",
    "\n",
    "def run_and_pickle(n):\n",
    "    numberOfFeatures = n\n",
    "    print \"Running for\", n\n",
    "    # They are a list of lists where each list represents a document as a collection of n frequent words.\n",
    "    features_fiction = []\n",
    "    features_nonfiction = []\n",
    "\n",
    "    print \"Fiction:\"\n",
    "    k = 0\n",
    "    for (n, folder) in fiction:\n",
    "        if k % 500 == 0:\n",
    "            print k\n",
    "        k += 1\n",
    "        with open('../Dataset/' + folder + '/' + n) as f:\n",
    "            text = f.read()\n",
    "            features_fiction.append(get_features(text, numberOfFeatures))\n",
    "\n",
    "    print \"\\n\\nNonFiction:\"\n",
    "    k = 0\n",
    "    for (n, folder) in nonfiction:\n",
    "        if k % 500 == 0:\n",
    "            print k\n",
    "        k += 1\n",
    "        with open('../Dataset/' + folder + '/' + n) as f:\n",
    "            text = f.read()\n",
    "            features_nonfiction.append(get_features(text, numberOfFeatures))\n",
    "            \n",
    "    # Pickling the results:\n",
    "    with open('./features/no_stopwords/fiction_'+str(numberOfFeatures)+'.pickle', 'wb') as f:\n",
    "        pickle.dump(features_fiction, f)\n",
    "    with open('./features/no_stopwords/non_fiction_'+str(numberOfFeatures)+'.pickle', 'wb') as f:\n",
    "        pickle.dump(features_nonfiction, f)\n",
    "        \n",
    "# run_and_pickle(50)\n",
    "# run_and_pickle(100)\n",
    "# run_and_pickle(500)\n",
    "# run_and_pickle(1000)\n",
    "# run_and_pickle(3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
