{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def pickle_load(n):\n",
    "    with open('./features/Model1_length1/fiction_' + str(n) + '.pickle', 'rb') as f:\n",
    "        fic = pickle.load(f)\n",
    "\n",
    "    with open('./features/Model1_length1/non_fiction_' + str(n) + '.pickle', 'rb') as f:\n",
    "        nonfic = pickle.load(f)\n",
    "\n",
    "    return fic, nonfic\n",
    "\n",
    "fic50, nonfic50     = pickle_load(50)\n",
    "fic100, nonfic100   = pickle_load(100)\n",
    "fic500, nonfic500   = pickle_load(500)\n",
    "fic1000, nonfic1000 = pickle_load(1000)\n",
    "fic3000, nonfic3000 = pickle_load(3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes:\n",
    "\n",
    "#### Model 1: All words len(word) > 1\n",
    "\n",
    "NOTE: cross_val_score and grid_search use stratified k-fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Different smoothing parameters in Naive Bayes.\n",
    "'''\n",
    "\n",
    "# New better version:\n",
    "def my_NB(X):\n",
    "    # Getting labels ready: 1 is for Fiction, 0 is for NonFiction\n",
    "    labels = [1]*4558 + [0]*4558\n",
    "    \n",
    "    tuned_parameters = {'alpha': [1.0, 1e-3, 1e-6, 1e-12, 1e-18]}\n",
    "    \n",
    "    metrics = ['precision', 'f1', 'accuracy', 'recall']\n",
    "    \n",
    "    # List of 4 dictionaries, where each dictionary represents all the results for that particular best model.\n",
    "    models = []\n",
    "    \n",
    "    for score in metrics:\n",
    "        model = {}\n",
    "        nb = MultinomialNB()\n",
    "        print \"Running for \", score\n",
    "        clf = GridSearchCV(nb, tuned_parameters, cv=10, scoring=score, verbose=1)\n",
    "        clf.fit(X, labels)\n",
    "        print \"\\nBest parameters for \" + score + \": \" + str(clf.best_estimator_)\n",
    "        print \"Best score achieved for \" + score + \": \" + str(clf.best_score_)\n",
    "        best_nb = clf.best_estimator_\n",
    "        # Now that I have the best parameters for each metric, running SVM for those specific parameters to obtain \n",
    "        # all values.\n",
    "        for s in metrics:\n",
    "            print \"Running the best \" + score + \" model for \" + s + \"..\"\n",
    "            model[s] = np.array(cross_val_score(best_nb, X, labels, cv=10, scoring=s))\n",
    "        \n",
    "        print \"For \", score \n",
    "        print model\n",
    "        print \"\\n\\n\"\n",
    "        models.append(model)\n",
    "\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4558\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "'''\n",
    "For 50:\n",
    "'''\n",
    "# Getting it ready for Count Vectorizer:\n",
    "countvec_fiction50 = []\n",
    "countvec_nonfiction50 = []\n",
    "\n",
    "for doc in fic50:\n",
    "    temp = ' '.join(doc)\n",
    "    countvec_fiction50.append(temp)\n",
    "    \n",
    "for doc in nonfic50:\n",
    "    temp = ' '.join(doc)\n",
    "    countvec_nonfiction50.append(temp)\n",
    "\n",
    "print len(countvec_fiction50) \n",
    "fiction_plus_nonfiction50 = countvec_fiction50 + countvec_nonfiction50\n",
    "    \n",
    "vectorizer50 = CountVectorizer()\n",
    "X50 = vectorizer50.fit_transform(fiction_plus_nonfiction50)\n",
    "\n",
    "\n",
    "'''\n",
    "For 100:\n",
    "'''\n",
    "# Getting it ready for Count Vectorizer:\n",
    "countvec_fiction100 = []\n",
    "countvec_nonfiction100 = []\n",
    "\n",
    "for doc in fic100:\n",
    "    temp = ' '.join(doc)\n",
    "    countvec_fiction100.append(temp)\n",
    "    \n",
    "for doc in nonfic100:\n",
    "    temp = ' '.join(doc)\n",
    "    countvec_nonfiction100.append(temp)\n",
    "    \n",
    "fiction_plus_nonfiction100 = countvec_fiction100 + countvec_nonfiction100\n",
    "\n",
    "vectorizer100 = CountVectorizer()\n",
    "X100 = vectorizer100.fit_transform(fiction_plus_nonfiction100)\n",
    "\n",
    "\n",
    "'''\n",
    "For 500:\n",
    "'''\n",
    "# Getting it ready for Count Vectorizer:\n",
    "countvec_fiction500 = []\n",
    "countvec_nonfiction500 = []\n",
    "\n",
    "for doc in fic500:\n",
    "    temp = ' '.join(doc)\n",
    "    countvec_fiction500.append(temp)\n",
    "    \n",
    "for doc in nonfic500:\n",
    "    temp = ' '.join(doc)\n",
    "    countvec_nonfiction500.append(temp)\n",
    "    \n",
    "fiction_plus_nonfiction500 = countvec_fiction500 + countvec_nonfiction500\n",
    "    \n",
    "vectorizer500 = CountVectorizer()\n",
    "X500 = vectorizer500.fit_transform(fiction_plus_nonfiction500)\n",
    "\n",
    "\n",
    "'''\n",
    "For 1000:\n",
    "'''\n",
    "# Getting it ready for Count Vectorizer:\n",
    "countvec_fiction1000 = []\n",
    "countvec_nonfiction1000 = []\n",
    "\n",
    "for doc in fic1000:\n",
    "    temp = ' '.join(doc)\n",
    "    countvec_fiction1000.append(temp)\n",
    "    \n",
    "for doc in nonfic1000:\n",
    "    temp = ' '.join(doc)\n",
    "    countvec_nonfiction1000.append(temp)\n",
    "    \n",
    "fiction_plus_nonfiction1000 = countvec_fiction1000 + countvec_nonfiction1000\n",
    "\n",
    "vectorizer1000 = CountVectorizer()\n",
    "X1000 = vectorizer1000.fit_transform(fiction_plus_nonfiction1000)\n",
    "\n",
    "'''\n",
    "For 3000:\n",
    "'''\n",
    "# Getting it ready for Count Vectorizer:\n",
    "countvec_fiction3000 = []\n",
    "countvec_nonfiction3000 = []\n",
    "\n",
    "for doc in fic3000:\n",
    "    temp = ' '.join(doc)\n",
    "    countvec_fiction3000.append(temp)\n",
    "    \n",
    "for doc in nonfic3000:\n",
    "    temp = ' '.join(doc)\n",
    "    countvec_nonfiction3000.append(temp)\n",
    "    \n",
    "fiction_plus_nonfiction3000 = countvec_fiction3000 + countvec_nonfiction3000\n",
    "    \n",
    "vectorizer3000 = CountVectorizer()\n",
    "X3000 = vectorizer3000.fit_transform(fiction_plus_nonfiction3000)\n",
    "\n",
    "\n",
    "# DONE with all cases.\n",
    "\n",
    "# print \"Set I) 50 words:\"\n",
    "# results_50 = my_NB(X50)\n",
    "# print \"\\n################################################################\\n\"\n",
    "# print \"Set II) 100 words:\"\n",
    "# results_100 = my_NB(X100)\n",
    "# print \"\\n################################################################\\n\"\n",
    "# print \"Set III) 500 words:\"\n",
    "# results_500 = my_NB(X500)\n",
    "# print \"\\n################################################################\\n\"\n",
    "# print \"Set IV) 1000 words:\"\n",
    "# results_1000 = my_NB(X1000)\n",
    "# print \"\\n################################################################\\n\"\n",
    "# print \"Set V) 3000 words:\"\n",
    "# results_3000 = my_NB(X3000)\n",
    "# print \"\\n################################################################\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.899606420458\n",
      "0.901239274724\n",
      "0.901239274724\n",
      "0.896187898781\n",
      "recall: 0.898174763833\n",
      "f1: 0.899606420458\n",
      "precision: 0.901848665776\n",
      "accuracy: 0.900272074417\n"
     ]
    }
   ],
   "source": [
    "##### CHECKING THE IMPLEMENTATION, and PRINTING USEFUL VALUES #####\n",
    "\n",
    "# NOTE: Because SVM was taking a really long time to run, I only optimised f1. So, results_50 has only one element\n",
    "#       in the list.\n",
    "'''\n",
    "This code should repeat the best values obtained for each metric.\n",
    "'''\n",
    "# metrics = ['precision', 'f1', 'accuracy', 'recall']\n",
    "# for score in metrics:\n",
    "#     print results_50[0][score].mean()\n",
    "\n",
    "'''\n",
    "But, we care about f1. It will always be the second element in the list resuls_50, but just confirming.\n",
    "'''\n",
    "for d in results_50:\n",
    "    print d['f1'].mean()\n",
    "'''\n",
    "So now that it's confirmed it's the second element in the list, concentrating on the model with the best f1:\n",
    "'''\n",
    "for m in results_50[1]:\n",
    "    print m + \": \" + str(results_50[0][m].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 50 words\n",
      "{'recall': array([ 0.88596491,  0.96929825,  0.97368421,  0.94298246,  0.93421053,\n",
      "        0.93640351,  0.89473684,  0.94078947,  0.87692308,  0.81538462]), 'f1': array([ 0.89977728,  0.94545455,  0.9527897 ,  0.93784079,  0.92008639,\n",
      "        0.92324324,  0.89571899,  0.91568837,  0.89162011,  0.848     ]), 'precision': array([ 0.91402715,  0.92275574,  0.93277311,  0.93275488,  0.90638298,\n",
      "        0.91044776,  0.8967033 ,  0.89189189,  0.90681818,  0.88333333]), 'accuracy': array([ 0.90131579,  0.94407895,  0.95175439,  0.9375    ,  0.91885965,\n",
      "        0.92214912,  0.89583333,  0.91337719,  0.89340659,  0.85384615])}\n",
      "recall: 0.917037786775    Std-Dev: 0.0461005297322\n",
      "f1: 0.913021941836    Std-Dev: 0.0292771566183\n",
      "precision: 0.909788832405    Std-Dev: 0.0156655812364\n",
      "accuracy: 0.913212116831    Std-Dev: 0.0273862775727\n",
      "\n",
      "\n",
      "For 100 words\n",
      "{'recall': array([ 0.88157895,  0.98245614,  0.96929825,  0.97149123,  0.93859649,\n",
      "        0.94517544,  0.9254386 ,  0.94078947,  0.89450549,  0.81978022]), 'f1': array([ 0.91259932,  0.96655879,  0.95670996,  0.96095445,  0.94065934,\n",
      "        0.9441402 ,  0.93466224,  0.93565976,  0.91977401,  0.8786808 ]), 'precision': array([ 0.94588235,  0.95116773,  0.94444444,  0.95064378,  0.94273128,\n",
      "        0.94310722,  0.94407159,  0.93058568,  0.94651163,  0.94670051]), 'accuracy': array([ 0.91557018,  0.96600877,  0.95614035,  0.96052632,  0.94078947,\n",
      "        0.94407895,  0.93530702,  0.93530702,  0.92197802,  0.88681319])}\n",
      "recall: 0.926911027569    Std-Dev: 0.0470280493122\n",
      "f1: 0.935039886145    Std-Dev: 0.0248243652414\n",
      "precision: 0.944584620817    Std-Dev: 0.00540181276687\n",
      "accuracy: 0.936251927897    Std-Dev: 0.0224267044048\n",
      "\n",
      "\n",
      "For 500 words\n",
      "{'recall': array([ 0.8245614 ,  0.97368421,  0.97149123,  0.97368421,  0.92324561,\n",
      "        0.9254386 ,  0.89473684,  0.92105263,  0.82417582,  0.74505495]), 'f1': array([ 0.88888889,  0.96732026,  0.96830601,  0.9704918 ,  0.94183445,\n",
      "        0.94301676,  0.9262202 ,  0.94276094,  0.88443396,  0.84119107]), 'precision': array([ 0.96410256,  0.96103896,  0.96514161,  0.96732026,  0.96118721,\n",
      "        0.96127563,  0.96      ,  0.96551724,  0.95419847,  0.96581197]), 'accuracy': array([ 0.89692982,  0.96710526,  0.96820175,  0.97039474,  0.94298246,\n",
      "        0.94407895,  0.92872807,  0.94407895,  0.89230769,  0.85934066])}\n",
      "recall: 0.897712550607    Std-Dev: 0.0728176143719\n",
      "f1: 0.927446435255    Std-Dev: 0.0407118707824\n",
      "precision: 0.962559392029    Std-Dev: 0.00365159297865\n",
      "accuracy: 0.931414835165    Std-Dev: 0.035385839325\n",
      "\n",
      "\n",
      "For 1000 words\n",
      "{'recall': array([ 0.82236842,  0.9627193 ,  0.97368421,  0.96052632,  0.92324561,\n",
      "        0.90570175,  0.87719298,  0.91008772,  0.8043956 ,  0.72967033]), 'f1': array([ 0.88547816,  0.9627193 ,  0.96837514,  0.96369637,  0.94500561,\n",
      "        0.93333333,  0.91428571,  0.9336333 ,  0.87769784,  0.8320802 ]), 'precision': array([ 0.95907928,  0.9627193 ,  0.96312364,  0.96688742,  0.96781609,\n",
      "        0.96270396,  0.95465394,  0.95842956,  0.96569921,  0.96793003]), 'accuracy': array([ 0.89364035,  0.9627193 ,  0.96820175,  0.96381579,  0.94627193,\n",
      "        0.93530702,  0.91776316,  0.93530702,  0.88791209,  0.85274725])}\n",
      "recall: 0.886959224986    Std-Dev: 0.075299229634\n",
      "f1: 0.921630495976    Std-Dev: 0.0421670242169\n",
      "precision: 0.962904243501    Std-Dev: 0.00419410743598\n",
      "accuracy: 0.926368565645    Std-Dev: 0.0361577228826\n",
      "\n",
      "\n",
      "For 3000 words\n",
      "{'recall': array([ 0.82017544,  0.96929825,  0.97807018,  0.96052632,  0.92324561,\n",
      "        0.9122807 ,  0.88815789,  0.9254386 ,  0.84615385,  0.74945055]), 'f1': array([ 0.87896592,  0.96296296,  0.96851249,  0.95633188,  0.94288914,\n",
      "        0.92857143,  0.91732729,  0.92849285,  0.89639115,  0.83578431]), 'precision': array([ 0.94683544,  0.95670996,  0.95913978,  0.95217391,  0.96338673,\n",
      "        0.94545455,  0.94847775,  0.93156733,  0.9529703 ,  0.94459834]), 'accuracy': array([ 0.8870614 ,  0.9627193 ,  0.96820175,  0.95614035,  0.94407895,\n",
      "        0.92982456,  0.91995614,  0.92872807,  0.9021978 ,  0.85274725])}\n",
      "recall: 0.897279737806    Std-Dev: 0.0692110349447\n",
      "f1: 0.921622942471    Std-Dev: 0.0393739156327\n",
      "precision: 0.950131408654    Std-Dev: 0.00850337391723\n",
      "accuracy: 0.925165558126    Std-Dev: 0.0344455263666\n"
     ]
    }
   ],
   "source": [
    "# Put this in the table: This is the best-f1-model scores.\n",
    "print \"For 50 words\"\n",
    "print results_50[1]\n",
    "for m in results_50[1]:\n",
    "    print m + \": \" + str(results_50[1][m].mean()) + \"    Std-Dev: \" + str(results_50[1][m].std())\n",
    "    \n",
    "print \"\\n\\nFor 100 words\"\n",
    "print results_100[1]\n",
    "for m in results_100[1]:\n",
    "    print m + \": \" + str(results_100[1][m].mean()) + \"    Std-Dev: \" + str(results_100[1][m].std())\n",
    "    \n",
    "print \"\\n\\nFor 500 words\"\n",
    "print results_500[1]\n",
    "for m in results_500[1]:\n",
    "    print m + \": \" + str(results_500[1][m].mean()) + \"    Std-Dev: \" + str(results_500[1][m].std())\n",
    "    \n",
    "print \"\\n\\nFor 1000 words\"\n",
    "print results_1000[1]\n",
    "for m in results_1000[1]:\n",
    "    print m + \": \" + str(results_1000[1][m].mean()) + \"    Std-Dev: \" + str(results_1000[1][m].std())\n",
    "    \n",
    "print \"\\n\\nFor 3000 words\"\n",
    "print results_3000[1]\n",
    "for m in results_3000[1]:\n",
    "    print m + \": \" + str(results_3000[1][m].mean()) + \"    Std-Dev: \" + str(results_3000[1][m].std())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
